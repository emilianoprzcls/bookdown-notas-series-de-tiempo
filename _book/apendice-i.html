<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Capítulo 13 Apendice I | Notas de Clase: Series de Tiempo</title>

    <meta name="author" content="Benjamín Oliva, Omar Alfaro-Rivera y Emiliano Pérez Caullieres" />
  
   <meta name="description" content="Trabajo siempre en proceso de mejora, para cualquier comentario o aclaración, contactar al correo benjov@ciencias.unam.mx o omarxalpha@gmail.com" />
   <meta name="generator" content="placeholder" />
  <meta property="og:title" content="Capítulo 13 Apendice I | Notas de Clase: Series de Tiempo" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Trabajo siempre en proceso de mejora, para cualquier comentario o aclaración, contactar al correo benjov@ciencias.unam.mx o omarxalpha@gmail.com" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 13 Apendice I | Notas de Clase: Series de Tiempo" />
  
  <meta name="twitter:description" content="Trabajo siempre en proceso de mejora, para cualquier comentario o aclaración, contactar al correo benjov@ciencias.unam.mx o omarxalpha@gmail.com" />
  
  <!-- JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
    <script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
    <script src="libs/bs3compat-0.4.0/transition.js"></script>
    <script src="libs/bs3compat-0.4.0/tabs.js"></script>
    <script src="libs/bs3compat-0.4.0/bs3compat.js"></script>
    <link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet" />
    <script src="libs/bs4_book-1.0.0/bs4_book.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script>

  <!-- CSS -->
  <style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
      <link rel="stylesheet" href="style.css" />
  
</head>

<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book">
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Notas de Clase: Series de Tiempo</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
      </form>

      <nav aria-label="Table of contents">
        <h2>Table of contents</h2>
        <div id="book-toc"></div>

        <div class="book-extra">
          <p><a id="book-repo" href="#">View book source <i class="fab fa-github"></i></a></li></p>
        </div>
      </nav>
    </div>
  </header>

  <main class="col-sm-12 col-md-9 col-lg-7" id="content">
<div id="apendice-i" class="section level1" number="13">
<h1><span class="header-section-number">Capítulo 13</span> Apendice I</h1>
<div id="estimador-de-mínimos-cuadrados-ordinarios-y-el-análisis-clásico-de-regresión" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Estimador de Mínimos Cuadrados Ordinarios y el análisis clásico de regresión</h2>
<p>El estimador de Mínimos Cuadrados Ordinarios (MCO) es el estimador básico en econometría o, propiamente dicho, en el análisis de regresión. Esta sección cubre las propiedades finitas del estimador de MCO, mismas que son validas para cualquier tamaño de muestra dado. El material cubierto es totalmente estándar.</p>
<p>Cualquier estudio econométrico inicia con un conjunto de proposiciones sobre algún fenómeno de la economía. Algunos ejemplos familiares son las ecuaciones de demanda, las funciones de producción y algunos otros modelos macroeconómicos. Así, la investigación empírica provee las estimaciones de los parámetros desconocidos en el modelo. La teoría especifica un conjunto de relaciones determinísticas sobre las variables.</p>
<p>Dichas relaciones sulen estudiarse mediante el análisis de regresión múltiple, el cual permite el estudio de la relación entre una <em>variable dependiente</em> y otras más denominadas <em>variables independientes</em>. En adelante, en general diremos que la forma de representar la relación entre la variable dependiente y las variables independientes, tendrá la siguiente notación:
<span class="math display" id="eq:emq1">\[\begin{eqnarray}
    y &amp; = &amp; f(x_1, x_2, \ldots , x_K) + \varepsilon \nonumber \\
    &amp; = &amp; x_1\beta_1 + x_2\beta_2 + \ldots + x_K\beta_K + \varepsilon
    \tag{11.1}
\end{eqnarray}\]</span></p>
<p>donde <span class="math inline">\(y\)</span> es la variable dependiente o <em>explicada</em>, el conjunto de variables dado por <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, …, <span class="math inline">\(x_K\)</span> son las variables independientes o <em>explicativas</em> y de la teoría tomamos la especificación descrita por <span class="math inline">\(f(x_1, x_2, \ldots, x_K)\)</span>. Esta función es comúnmente llamada la <em>ecuación de regresión poblacional</em> de <span class="math inline">\(y\)</span> en <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, …, <span class="math inline">\(x_K\)</span>. El término <span class="math inline">\(\varepsilon\)</span> es una <em>perturbación</em> aleatoria o error de estimación.</p>
<p>Este error existe por varias razones, principalmente, porque no esperamos capturar toda la influencia que existe o determina a una varaible económica en un modelo simplista como el que generalmente se formula en el análisis de regresión. Digamos, entonces, que existe un conjunto de información no observable que permite la existencia del término de error. Por ejemplo, existe una clara dificultad para obtener medidas razonables de cualidades como habilidades o capcidades de aprendizaje de un conjunto de individuos a los cuales, quizá, queremos medir su productividad. Por lo tanto, sólo podemos medir el efecto de aquellas variables o información que es cuantificable. El resto de la información la conoceremos como aquella que no es observable. Así, el término de error existe a razón de dicha información.</p>
<p>Implícitamente, estamos suponiendo que cada una de las observaciones en una muestra dada por <span class="math inline">\(\{y_i, x_{i1}, x_{i2}, \ldots, x_{iK} \}\)</span>, para <span class="math inline">\(i = 1, \ldots, n\)</span>, fue generada por un proceso subyacente descrito por:
<span class="math display" id="eq:emq2">\[\begin{equation}
    y_i = x_{i1}\beta_1 + x_{i2}\beta_2 + \ldots + x_{iK}\beta_K + \varepsilon_i
        \tag{11.2}
\end{equation}\]</span></p>
<p>Es decir, el valor observado de <span class="math inline">\(y_i\)</span> es igual a la suma de dos partes: una parte determinística, <span class="math inline">\(x_{i1}\beta_1 + x_{i2}\beta_2 + \ldots + x_{iK}\beta_K\)</span>, y una parte aleatoria, <span class="math inline">\(\varepsilon_i\)</span>. Dicho esto, el objetivo del análisis de regresión radica en estimar los parámetros desconocidos del modelo (<span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\beta_K\)</span>), validar la proposiciones teóricas usando los datos disponibles y predecir el valor de la variable <span class="math inline">\(y_i\)</span> mediante el uso del modelo estimado.</p>
<p>Sea <span class="math inline">\(\mathbf{X}_k\)</span> el vector columna de <span class="math inline">\(n\)</span> observaciones de la variable <span class="math inline">\(x_k\)</span>, donde <span class="math inline">\(k = 1, \ldots, K\)</span>, y que colocado en una matriz da por resultado un arreglo <span class="math inline">\(\mathbf{X}\)</span> de tamaño <span class="math inline">\(n \times K\)</span>. Es decir, cada una de las columnas de la siguiente matriz representa todas las observaciones de cada una de las variables:
<span class="math display" id="eq:emq3">\[\begin{equation}
    \left[
    \begin{array}{c c c c}
    \mathbf{X}_1 &amp; \mathbf{X}_2 &amp; \ldots &amp; \mathbf{X}_K
    \end{array}
    \right]
    =
    \left[
    \begin{array}{c c c c}
    x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1K}\\
    x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2K}\\
    x_{31} &amp; x_{32} &amp; \ldots &amp; x_{3K}\\
    \vdots &amp; \vdots &amp; \ldots &amp; \vdots\\
    x_{n1} &amp; x_{n2} &amp; \ldots &amp; x_{nK}\\
    \end{array}
    \right]
        \tag{11.4}
\end{equation}\]</span></p>
<p>En la mayoría de las veces vamos a asumir que existe una columna compuesta del número 1 (uno) en todas sus entradas, tal que, el paramétro <span class="math inline">\(\beta_1\)</span> es un término constante en el modelo. De esta forma la matriz anteriormente mostrada se puede ver como:
<span class="math display" id="eq:emq4">\[\begin{equation}
    \left[
    \begin{array}{c c c c}
    \mathbf{1} &amp; \mathbf{X}_2 &amp; \ldots &amp; \mathbf{X}_K
    \end{array}
    \right]
    =
    \left[
    \begin{array}{c c c c}
    1 &amp; x_{12} &amp; \ldots &amp; x_{1K}\\
    1 &amp; x_{22} &amp; \ldots &amp; x_{2K}\\
    1 &amp; x_{32} &amp; \ldots &amp; x_{3K}\\
    \vdots &amp; \vdots &amp; \ldots &amp; \vdots\\
    1 &amp; x_{n2} &amp; \ldots &amp; x_{nK}\\
    \end{array}
    \right]
        \tag{12.1}
\end{equation}\]</span></p>
<p>Adicionalmente, denotaremos a <span class="math inline">\(\mathbf{Y}\)</span> como un vector columna de <span class="math inline">\(n\)</span> observaciones (<span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(y_n\)</span>, en forma de columna), y a <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> como el vector columna de <span class="math inline">\(n\)</span> perturbaciones (<span class="math inline">\(\varepsilon_1\)</span>, <span class="math inline">\(\varepsilon_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\varepsilon_n\)</span>, en forma de columna). El modelo descrito en la ecuación (1) se puede escribir en su forma general como:</p>
<p><span class="math display" id="eq:emq5">\[\begin{equation}
{\bf{Y}} = {\bf {X}}_1\beta_1 + {\bf{X}}_2\beta_2 + \ldots + {\bf{X}}_K\beta_K + {\boldsymbol{\varepsilon}}
    \tag{12.2}
\end{equation}\]</span></p>
<p>Ecuación que podemos rescribir como:
<span class="math display" id="eq:emq6">\[\begin{equation}
\mathbf{Y}
=
\left[
\begin{array}{c c c c}
\mathbf{X}_1 &amp; \mathbf{X}_2 &amp; \ldots &amp; \mathbf{X}_K
\end{array}
\right]
\left[
\begin{array}{c}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_K
\end{array}
\right]
+
\
\boldsymbol{\varepsilon}
=
\boldsymbol{X\beta} + \boldsymbol{\varepsilon}
   \tag{12.3}
\end{equation}\]</span>
\end{equation}</p>
<p>Adicionalmente, de ahora en delante diremos que la regresión lineal dada por <span class="math inline">\(y_i = x_{i1}\beta_1 + x_{i2}\beta_2 + \ldots + x_{iK}\beta_K + \varepsilon_i\)</span>, se podrá escribir como:
<span class="math display" id="eq:emq7">\[\begin{equation}
y_i = (x_{i1}, x_{i2}, \ldots, x_{iK})(\beta_1, \beta_2, \ldots, \beta_K)&#39; + \varepsilon = \mathbf{X}&#39;_i \boldsymbol{\beta} + \varepsilon_i
   \tag{13.1}
\end{equation}\]</span></p>
<p>Así, los parámetros desconocidos, <span class="math inline">\(\boldsymbol{\beta}\)</span>, de la relación estocástica dada por <span class="math inline">\(y_i = \mathbf{X}&#39;_i \boldsymbol{\beta} + \varepsilon_i\)</span> son el objeto de la estimación. En este sentido distingamos que <span class="math inline">\(\boldsymbol{\beta}\)</span> y <span class="math inline">\(\varepsilon_i\)</span> son, respectivamente, el conjumto de los parámetros y el término de error de la población, y que, por lo tanto, denotaremos a las estimaciones relsultantes de una muestra como <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> y <span class="math inline">\(e_i\)</span>. Es decir, siempre que no podamos adquirir o conocer la iformación de todos los elementos de la población, nuestras aproximaciones muestrales se denotarán de forma distinta a las que refieran a la población.</p>
<p>Así, los principios de <em>regresión poblacional</em> y <em>regresión muestral</em> están dados por las fórmulas <span class="math inline">\(E[y_i|\mathbf{X}_i] = \mathbf{X}&#39;_i\boldsymbol{\beta}\)</span> y <span class="math inline">\(\hat{y}_i = \mathbf{X}&#39;_i\hat{\boldsymbol{\beta}}\)</span>, respectivamente. Donde <span class="math inline">\(\hat{y}_i\)</span> es el estimador de <span class="math inline">\(E[y_i|\mathbf{X}_i]\)</span>.</p>
<p>Por su parte, el término de error asociado será:
<span class="math display" id="eq:emq8">\[\begin{equation}
\varepsilon_i = y_i - {\bf{X}}&#39;_i \boldsymbol{\beta}
   \tag{13.2}
\end{equation}\]</span></p>
<p>si hablamos del caso poblacional o,
<span class="math display" id="eq:emq9">\[\begin{equation}
e_i = y_i - {\bf{X}}&#39;_i \hat{\boldsymbol{\beta}}
   \tag{13.3}
\end{equation}\]</span></p>
<p>cuando hagamos referencia al caso muestral. Es decir, nuestro estimador de <span class="math inline">\(\varepsilon_i\)</span> es <span class="math inline">\(e_i\)</span>. De lo dicho hasta ahora podemos escribir:
<span class="math display" id="eq:emq10">\[\begin{equation}
y_i = \mathbf{X}&#39;_i \boldsymbol{\beta} + \varepsilon_i = \mathbf{X}&#39;_i \hat{\boldsymbol{\beta}} + e_i
   \tag{13.4}
\end{equation}\]</span></p>
<p>Intuitivamente, la ecuación (36) significa que siempre que poseamos una muestra de los elementos de la población, podremos explicar una parte de la variable dependiente, no su totalidad. En este sentido, el análisis de regresión consiste en un proceso de ajuste a la variable dependiente. Está es la idea que da origen al <span class="math inline">\(R^2\)</span> y otras medidas de bondad de ajuste, mismas que más adelante en el curso analizaremos.</p>
<p>Regresando a la discusión central de esta sección, el método de MCO, en consecuencia, resulta en encontrar la combinación de parámetros <span class="math inline">\(\hat\beta\)</span> que permita minimizar la suma de los residuales al cuadrado dada por:
<span class="math display" id="eq:emq11">\[\begin{equation}
\sum^{n}_{i=1}{e^2_i} = \sum^{n}_{i=1}{(y_i - {\bf X}&#39;_i\hat{\boldsymbol{\beta}})^2}
   \tag{13.5}
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> denota el vector de estimadores <span class="math inline">\(\hat{\beta}_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\hat{\beta}_K\)</span>. En términos matriciales, dado que <span class="math inline">\((e_1, e_2, \ldots, e_n)&#39;(e_1, e_2, \ldots, e_n) = {\mathbf{e&#39;e}}\)</span>, el problema del método de MCO consiste en:
<span class="math display" id="eq:emq12">\[\begin{equation}
Minimizar_{\hat{\boldsymbol \beta}} S(\hat{\boldsymbol \beta}) = Minimizar_{\hat{\boldsymbol \beta}} \mathbf{e&#39;e}
   \tag{13.6}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:emq13">\[\begin{equation}
= Minimizar_{\hat{\boldsymbol \beta}} (\mathbf{Y}-\mathbf{X}\hat{\boldsymbol \beta})&#39;(\mathbf{Y}-\mathbf{X}\hat{\boldsymbol \beta})
   \tag{13.7}
\end{equation}\]</span></p>
<p>Expandiendo la expresión <span class="math inline">\(\mathbf{e&#39;e}\)</span> obtenemos:
<span class="math display" id="eq:emq14">\[\begin{equation}
\mathbf{e&#39;e} = \mathbf{Y&#39;Y} - 2 \mathbf{Y&#39;X} \hat{\boldsymbol \beta} + \hat{\boldsymbol \beta}&#39; \mathbf{X&#39;X}\hat{\boldsymbol \beta}
   \tag{13.8}
\end{equation}\]</span></p>
<p>De esta forma obtenemos que las condiciones necesarias de un mínimo son:
<span class="math display" id="eq:emq15">\[\begin{equation}
\frac{\partial S(\hat{\boldsymbol \beta})}{\partial \hat{\boldsymbol \beta}} = -2{\bf{X&#39;Y}} + 2{\bf{X&#39;X}} \hat{\boldsymbol{\beta}} = \bf{0}
   \tag{13.9}
\end{equation}\]</span></p>
<p>De ecuación anterior obtenemos para la solución del problema del mínimo a las ecuaciones siguientes conocidas como dadas por:
<span class="math display" id="eq:emq16">\[\begin{equation}
\bf{X&#39;X}\hat{\boldsymbol \beta} = \bf{X&#39;Y}
   \tag{13.10}
\end{equation}\]</span></p>
<p>Notemos que dichas ecuaciones normales son en realidad un sistema de ecuaciones de <span class="math inline">\(K\)</span> variables o incógnitas. Por un lado, recordemos que <span class="math inline">\(\mathbf X\)</span> es una matriz de dimensión <span class="math inline">\(n \times K\)</span>, con lo cual <span class="math inline">\(\mathbf X&#39;\)</span> es de dimensión <span class="math inline">\(K \times n\)</span>. Así, el producto <span class="math inline">\(\mathbf{X&#39;X}\)</span> dará como resultado una matriz cuadrada de dimensión <span class="math inline">\(K \times K\)</span>. Por otro lado, sabemos que <span class="math inline">\(\mathbf Y\)</span> es un vector de tamaño <span class="math inline">\(n \times 1\)</span>, con lo cual el producto <span class="math inline">\(\mathbf{X&#39;Y}\)</span> da como resultado un vector de dimención <span class="math inline">\(K \times 1\)</span>. En conclusión, el sistema de ecuaciones normales consiste en <span class="math inline">\(K\)</span> ecuaciones con <span class="math inline">\(K\)</span> incógnitas (<span class="math inline">\(\hat{\beta}_1, \ldots, \hat{\beta}_K\)</span>). Ante este hecho, existen múltiples formas mediante las cuales se puede solucionar dicho sistema, sin embargo en nuesto caso seguiremos el siguiente procedimiento.</p>
<p>Si la inversa de la matriz <span class="math inline">\(\mathbf{X&#39;X}\)</span> existe (recuerde que el procedimiento de MCO tradicional supone que <span class="math inline">\(\mathbf{X}\)</span> es de rango completo), la solución esta dada por la siguiente expresión:
<span class="math display" id="eq:emq17">\[\begin{equation}
\hat{\boldsymbol \beta} = (\mathbf{X&#39;X})^{-1}\mathbf{X&#39;Y}
   \tag{13.11}
\end{equation}\]</span></p>
<p>Esta expresión, a pesar de ser en apariencia compleja se puede ver como un conjunto de sumas. En general hemos supuesto que nuestra regresión a estimar esta descrita por la eccuación: <span class="math inline">\(y_i = x_{i1}\beta_1 + x_{i2}\beta_2 + \ldots + x_{iK}\beta_K + \varepsilon_i\)</span>, de esta forma tenemos <span class="math inline">\(K\)</span> variables independientes es nuestra regresión.</p>
<p>Ahora bien, si denotamos a <span class="math inline">\(\mathbf{X}_k\)</span> como el vector columna formado por todas las observaciones de la muestra (<span class="math inline">\(i = 1, 2, \ldots, n\)</span>) para la variable <span class="math inline">\(k\)</span>, podemos decir que la matriz <span class="math inline">\(\mathbf X\)</span> que contiene todas las variable independientes se forma por la concatenación de cada uno de los <span class="math inline">\(K\)</span> vectores columna. Dicho esto, podemos ver que las matrices <span class="math inline">\(\mathbf{X}\)</span> y <span class="math inline">\(\mathbf{X&#39;}\)</span> se pueden expresar como:</p>
<p><span class="math display">\[
\mathbf{X} =
\left[ \begin{array}{ccccc}
x_{11} &amp; x_{12} &amp; x_{13} &amp; \ldots &amp; x_{1K} \\
x_{21} &amp; x_{22} &amp; x_{23} &amp; \ldots &amp; x_{2K} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; \ldots &amp; x_{nK}
\end{array} \right] =
\left[ \begin{array}{ccccc}
\mathbf{X}_1 &amp; \mathbf{X}_2&amp; \mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_K
\end{array} \right]
\]</span></p>
<p><span class="math display">\[
\bf{X&#39;} =
\left[ \begin{array}{ccccc}
x_{11} &amp; x_{21} &amp; x_{31} &amp; \ldots &amp; x_{n1} \\
x_{12} &amp; x_{22} &amp; x_{32} &amp; \ldots &amp; x_{n2} \\
x_{13} &amp; x_{23} &amp; x_{33} &amp; \ldots &amp; x_{n3} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
x_{1K} &amp; x_{2K} &amp; x_{3K} &amp; \ldots &amp; x_{nK}
\end{array} \right] =
\left[ \begin{array}{c}
\mathbf{X}_1&#39; \\
\mathbf{X}_2&#39;\\
\mathbf{X}_3&#39;\\
\vdots \\
\mathbf{X}_K&#39;
\end{array} \right]
\]</span></p>
<p>Si suponemos que nuestra regresión tiene una constante, la especificación sería: <span class="math inline">\(y_i = \beta_1 + x_{i2}\beta_2 + \ldots + x_{iK}\beta_K + \varepsilon_i\)</span>, con unas matrices <span class="math inline">\(\mathbf X\)</span> y <span class="math inline">\(\mathbf X&#39;\)</span> dadas:
<span class="math display">\[
\mathbf{X} =
\left[ \begin{array}{ccccc}
1 &amp; x_{12} &amp; x_{13} &amp; \ldots &amp; x_{1K} \\
1 &amp; x_{22} &amp; x_{23} &amp; \ldots &amp; x_{2K} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{n2} &amp; x_{n3} &amp; \ldots &amp; x_{nK}
\end{array} \right] =
\left[ \begin{array}{ccccc}
\mathbf{1_n} &amp; \mathbf{X}_2&amp; \mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_K
\end{array} \right]
\]</span></p>
<p><span class="math display">\[
\bf{X&#39;} =
\left[ \begin{array}{ccccc}
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1 \\
x_{12} &amp; x_{22} &amp; x_{32} &amp; \ldots &amp; x_{n2} \\
x_{13} &amp; x_{23} &amp; x_{33} &amp; \ldots &amp; x_{n3} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
x_{1K} &amp; x_{2K} &amp; x_{3K} &amp; \ldots &amp; x_{nK}
\end{array} \right] =
\left[ \begin{array}{c}
\mathbf{1_n}&#39; \\
\mathbf{X}_2&#39;\\
\mathbf{X}_3&#39;\\
\vdots \\
\mathbf{X}_K&#39;
\end{array} \right]
\]</span></p>
<p>Donde <span class="math inline">\(\mathbf{1_n}\)</span> es un vector columna compuesto de 1’s (unos). Retomando (14), desarrollemos cada uno de los casos anteriores, así obtenemos lo siguiente para el caso general:
<span class="math display">\[
\mathbf{X&#39;X} =
\left[ \begin{array}{c}
\mathbf{X}_1&#39; \\
\mathbf{X}_2&#39; \\
\mathbf{X}_3&#39;\\
\vdots \\
\mathbf{X}_K&#39;
\end{array} \right]
\left[ \begin{array}{ccccc}
\mathbf{X}_1 &amp; \mathbf{X}_2 &amp; \mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_K
\end{array} \right]
\]</span></p>
<p><span class="math display">\[
\left[ \begin{array}{ccccc}
\mathbf{X}_1&#39;\bf{X}_1 &amp; \mathbf{X}_1&#39;\mathbf{X}_2 &amp; \mathbf{X}_1&#39;\mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_1&#39;\mathbf{X}_K \\
\mathbf{X}_2&#39;\mathbf{X}_1 &amp; \mathbf{X}_2&#39;\mathbf{X}_2 &amp; \mathbf{X}_2&#39;\mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_2&#39;\mathbf{X}_K \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\mathbf{X}_K&#39;\mathbf{X}_1 &amp; \mathbf{X}_K&#39;\mathbf{X}_2 &amp; \mathbf{X}_K&#39;\mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_K&#39;\mathbf{X}_K
\end{array} \right]
\]</span></p>
<p>Por lo tanto, obtenemos que:</p>
<p><span class="math display">\[
\mathbf{X&#39;X} =
\left[ \begin{array}{ccccc}
\sum^{n}_{i=1}{x_{i1}^2} &amp; \sum^{n}_{i=1}{x_{i1}x_{i2}} &amp; \sum^{n}_{i=1}{x_{i1}x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{i1}x_{iK}}\\
\sum^{n}_{i=1}{x_{i2}x_{i1}} &amp; \sum^{n}_{i=1}{x^{2}_{i2}} &amp; \sum^{n}_{i=1}{x_{i2}x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{i2}x_{iK}}\\
\sum^{n}_{i=1}{x_{i3}x_{i1}} &amp; \sum^{n}_{i=1}{x_{i3}x_{i2}} &amp; \sum^{n}_{i=1}{x^2_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{i3}x_{iK}}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\sum^{n}_{i=1}{x_{iK}x_{i1}} &amp; \sum^{n}_{i=1}{x_{iK}x_{i2}} &amp; \sum^{n}_{i=1}{x_{iK}x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x^2_{iK}}
\end{array} \right]
\]</span></p>
<p>Por otro lado, cuando supongamos que existe un término constante:</p>
<p><span class="math display">\[
\mathbf{X&#39;X} =
\left[ \begin{array}{ccccc}
n &amp; \sum^{n}_{i=1}{x_{i2}} &amp; \sum^{n}_{i=1}{x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{iK}}\\
\sum^{n}_{i=1}{x_{i2}} &amp; \sum^{n}_{i=1}{x^{2}_{i2}} &amp; \sum^{n}_{i=1}{x_{i2}x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{i2}x_{iK}}\\
\sum^{n}_{i=1}{x_{i3}} &amp; \sum^{n}_{i=1}{x_{i3}x_{i2}} &amp; \sum^{n}_{i=1}{x^2_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{i3}x_{iK}}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\sum^{n}_{i=1}{x_{iK}} &amp; \sum^{n}_{i=1}{x_{iK}x_{i2}} &amp; \sum^{n}_{i=1}{x_{iK}x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x^2_{iK}}
\end{array} \right]
\]</span></p>
<p>Adicionalmente, el producto <span class="math inline">\(\mathbf{X&#39;Y}\)</span>, en el caso general, se puede expresar como:</p>
<p><span class="math display">\[
\mathbf{X&#39;Y}
=
\left[ \begin{array}{c}
\mathbf{X}_1&#39; \\
\mathbf{X}_2&#39; \\
\mathbf{X}_3&#39;\\
\vdots \\
\mathbf{X}_K&#39;
\end{array} \right] \mathbf{Y}
=
\left[ \begin{array}{c}
\mathbf{X}_1&#39;\mathbf{Y}\\
\mathbf{X}_2&#39;\mathbf{Y}\\
\mathbf{X}_3&#39;\mathbf{Y}\\
\vdots\\
\mathbf{X}_K&#39;\mathbf{Y}
\end{array} \right]
=
\left[ \begin{array}{c}
\sum^{n}_{i=1}{x_{i1}y_{i}}\\
\sum^{n}_{i=1}{x_{i2}y_{i}}\\
\sum^{n}_{i=1}{x_{i3}y_{i}}\\
\vdots\\
\sum^{n}_{i=1}{x_{iK}y_{i}}
\end{array} \right]
\]</span></p>
<p>Si el modelo supone la existencia de un término constante, dicho producto se expresa como:</p>
<p><span class="math display">\[
\mathbf{X&#39;Y}
=
\left[ \begin{array}{c}
\mathbf{1_n}&#39; \\
\mathbf{X}_2&#39; \\
\mathbf{X}_3&#39;\\
\vdots \\
\mathbf{X}_K&#39;
\end{array} \right] \mathbf{Y}
=
\left[ \begin{array}{c}
\mathbf{1_n}&#39;\mathbf{Y}\\
\mathbf{X}_2&#39;\mathbf{Y}\\
\mathbf{X}_3&#39;\mathbf{Y}\\
\vdots\\
\mathbf{X}_K&#39;\mathbf{Y}
\end{array} \right]
=
\left[ \begin{array}{c}
\sum^{n}_{i=1}{y_{i}}\\
\sum^{n}_{i=1}{x_{i2}y_{i}}\\
\sum^{n}_{i=1}{x_{i3}y_{i}}\\
\vdots\\
\sum^{n}_{i=1}{x_{iK}y_{i}}
\end{array} \right]
\]</span></p>
<p>Finalmente, para que esta solución dada para el procedimiento de MCO sea un mínimo debemos buscar las condiciones de segundo orden:
<span class="math display" id="eq:emq18">\[\begin{equation}
\frac{\partial^2 S(\hat{\boldsymbol \beta})}{\partial \hat{\boldsymbol \beta} \partial \hat{\boldsymbol \beta&#39;}} = 2 \mathbf{X&#39;X}
   \tag{13.12}
\end{equation}\]</span></p>
<p>donde la matriz <span class="math inline">\(\mathbf{X&#39;X}\)</span> debe ser positiva definida para que la solución de MCO sea un mínimo. Sea <span class="math inline">\(q = \mathbf{c&#39;X&#39;Xc}\)</span> para algún vector <span class="math inline">\(\mathbf{c}\)</span> distinto de cero. Entonces:</p>
<p><span class="math display">\[
q = \mathbf{v&#39;v} = \sum_{i=1}^{n}{v_i^2} \textrm{, donde } \mathbf{v = Xc}
\]</span></p>
<p>Así, <span class="math inline">\(q\)</span> es positivo. Si <span class="math inline">\(\mathbf v\)</span> fuera cero, entonces existe una combinación lineal de las columnas de <span class="math inline">\(\mathbf X\)</span> que da como resultado cero, lo cual contradice el supuesto de que <span class="math inline">\(\mathbf X\)</span> es de rango completo. En todos los casos, si <span class="math inline">\(\mathbf X\)</span> es de rango completo, entonces la solución del método de MCO, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, es la única que mínimiza la suma de los residuales al cuadrado.</p>
<p>Finalmente, no debemos perder de vista que lo aqui espresado tiene un objeto, mostrar como este procedimiento de MCO es valido cuando suponemos regresiones del tipo:
<span class="math display" id="eq:emq19">\[\begin{equation}
y_t = \beta_0 + y_{t-1}\beta_1 + \ldots + y_{t-\tau}\beta_\tau + \varepsilon_t
   \tag{13.13}
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(t = 0, 1, 2, \ldots, T\)</span> es un índice del tiempo, la variable dependiente es <span class="math inline">\(y_t\)</span> y las variables independientes son la misma variable independiente, pero en forma rezagada. Así, podemos definir un vector columna <span class="math inline">\(\mathbf{Y}_{t-k}\)</span> el vector columna de <span class="math inline">\(T\)</span> observaciones de la variable dependiente rezagada <span class="math inline">\(k\)</span> veces, donde <span class="math inline">\(k = 1, \ldots, \tau\)</span>, y que colocado en una matriz da por resultado un arreglo <span class="math inline">\(\mathbf{X}\)</span> de tamaño <span class="math inline">\(T-\tau \times \tau+1\)</span>.</p>
</div>
<div id="estimación-por-el-método-de-máxima-verosimilitud-mv" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Estimación por el método de Máxima Verosimilitud (MV)</h2>
<p>Ahora analicemos otro método de estimación que tiene más uso en series de tiempo: MV. Iniciemos con algo de notación. La función de densidad de probabilidad de una variable aleatoria <span class="math inline">\(y\)</span>, condicional en un conjunto de parámetros, <span class="math inline">\(\boldsymbol{\theta}\)</span>, la denotaremos como <span class="math inline">\(f(y|\boldsymbol{\theta})\)</span>. Dicha función identifica el mecanismo generador de datos subyacente a la muestra observable y al mismo tiempo prove una descripción matemática de los datos que el proceso generará.</p>
<p>Por otro lado, la función de densidad conjunta de n observaciones <em>independientes e idénticamente distribuidas</em> (i.i.d) está dada por el siguiente producto de las funciones de densidad individuales:
<span class="math display" id="eq:emq20">\[\begin{equation}
f(y_1, y_2, \ldots, y_n |\boldsymbol{\theta}) = \prod_{i=1}^n f(y_i|\boldsymbol{\theta}) = L(\boldsymbol{\theta} | \boldsymbol{y})
   \tag{13.14}
\end{equation}\]</span></p>
<p>A esta función de densidad conjunta se le conoce como <em>Función de Verosimilitud</em>, la cual se define como una función del vector de parámetros, <span class="math inline">\(\boldsymbol{\theta}\)</span>, donde <span class="math inline">\(\boldsymbol{y}\)</span> indica la familia de observaciones en la muestra de datos. Notemos que la función de densidad conjunta la hemos escrito como una función de los datos observados condicional en los parámetros a estimar. Sin embargo, por otro lado también hemos dicho que la función de verosimilitud, aquella que es ídentica a la función de densidad conjunta, es una función de los parámetros condicional en los datos observados. Aunque ambas funciones son la misma cabe hace enfasís de que en la segunda buscamos aquellos parámetros que máximizan la función de verosimilitud condicional en los datos observados.</p>
<p>Ahora bien, el procedimiento de máxima verosimilitud, por simplicidad, se estima aplicando la función logarítmo natural a <span class="math inline">\(L(\boldsymbol{\theta} | \boldsymbol{y})\)</span>. Derivado de que la función logarítmo natural es monótona, ésta preserva el orden y con ello el valorque máximiza a la función. De esta forma escribiremos que la función será:
<span class="math display" id="eq:emq21">\[\begin{equation}
ln(L(\boldsymbol{\theta} | \boldsymbol{y})) = ln(\prod_{i=1}^n f(y_i|\boldsymbol{\theta})) = \sum_{i = 1}^{n} ln(f(y_i|\boldsymbol{\theta}))
   \tag{13.15}
\end{equation}\]</span></p>
<p>Así, por simplicidad diremos que denotaremos a el logarítmo de la función de densidad conjunta como:</p>
<p><span class="math display" id="eq:emq22">\[\begin{equation}
ln(L(\boldsymbol{\theta} | \boldsymbol{y})) = l(\boldsymbol{\theta} | \boldsymbol{y})
   \tag{13.16}
\end{equation}\]</span></p>
<p>Dicho lo anterior, el objetivo de esta sección es mostrar el procedimiento de estimación de Máxima Verosimilitud aplicado a una regresión lineal. Retomemos la idea de que en nuestra ecuación de regresión lineal: <span class="math inline">\(y_i = \mathbf{X}_i&#39;\boldsymbol \beta + \varepsilon_i\)</span> para <span class="math inline">\(i = 1, \ldots, n\)</span>, el término de error <span class="math inline">\(\varepsilon_i\)</span> se distribuye como una normal con media cero y varianza constante, <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[
\varepsilon_i \sim \mathbf{N}(0, \sigma^2)
\]</span></p>
<p>Asimismo, en genral hemos dicho que, visto como vector, el término de error tiene una distribución de la forma:
<span class="math display">\[
\boldsymbol \varepsilon \sim \mathbf{N}(\mathbf{0}, \sigma^2 \mathbf{I}_n)
\]</span></p>
<p>Obsérvese que la forma de la varianza del término de error: <span class="math inline">\(Var[\boldsymbol \varepsilon | \mathbf X] = \sigma^2 \mathbf{I}_n\)</span>, implica que la distribución de cada una de las <span class="math inline">\(\varepsilon_i\)</span> es independiente, de tel forma que la función de densidad esta dada por:
<span class="math display">\[
f(\varepsilon_i | \boldsymbol \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{1}{2} \frac{(\varepsilon_i - 0)^2}{\sigma^2}}
\]</span></p>
<p>Sustituyendo la definición del término de error obetenemos la siguiente expresión:
<span class="math display" id="eq:emq23">\[\begin{equation}
f(\varepsilon_i | \boldsymbol \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{1}{2} \frac{(\mathbf{X}_i&#39;\boldsymbol \beta - y_i)^2}{\sigma^2}}
   \tag{13.17}
\end{equation}\]</span></p>
<p>Donde el vector <span class="math inline">\(\boldsymbol \theta\)</span> se compone de el vector <span class="math inline">\(\boldsymbol \beta\)</span> y <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Por lo tanto, la función de verosimilitud asociada a este caso está dada por la siguiente expresión:
<span class="math display" id="eq:emq24">\[\begin{equation}
L(\boldsymbol{\theta} | \boldsymbol{\varepsilon}) = \prod_{i=1}^n f(\varepsilon_i|\boldsymbol{\theta}) = \prod_{i=1}^n\frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{1}{2} \frac{(\mathbf{X}_i&#39;\boldsymbol \beta - y_i)^2}{\sigma^2}}
   \tag{13.18}
\end{equation}\]</span></p>
<p>Esta última ecuación, en su forma logarítmica, se puede expresar como:
<span class="math display" id="eq:emq25">\[\begin{eqnarray}
l(\boldsymbol{\theta} | \boldsymbol{\varepsilon})
&amp; = &amp;
\sum_{i=1}^n{ \left[ ln(1) - ln(\sqrt{2 \pi \sigma^2}) - \frac{1}{2} \frac{(y_i - \mathbf{X}_i&#39;\boldsymbol \beta)^2}{\sigma^2} \right]} \nonumber \\
&amp; = &amp;
\sum_{i=1}^n{ \left[- \frac{1}{2}ln(2 \pi \sigma^2) - \frac{1}{2} \frac{(y_i - \mathbf{X}_i&#39;\boldsymbol \beta)^2}{\sigma^2} \right]} \nonumber \\
&amp; = &amp;
- \frac{1}{2} \sum_{i=1}^n{ \left[ln(2 \pi \sigma^2) + \frac{(y_i - \mathbf{X}_i&#39;\boldsymbol \beta)^2}{\sigma^2} \right]} \nonumber \\
&amp; = &amp;
- \frac{1}{2} \left[ \sum_{i=1}^n{ \left[ln(2 \pi \sigma^2) \right]} + \sum_{i=1}^n{ \left[\frac{(y_i - \mathbf{X}_i&#39;\boldsymbol \beta)^2}{\sigma^2} \right]} \right] \nonumber \\\nonumber \\
&amp; = &amp;
- \frac{1}{2} \left[ n \times ln(2 \pi \sigma^2) + \frac{1}{\sigma^2} \sum_{i=1}^n{ \left[(y_i - \mathbf{X}_i&#39;\boldsymbol \beta)^2\right]} \right] \nonumber \\
&amp; = &amp;
- \frac{n}{2}ln(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \boldsymbol \varepsilon&#39; \boldsymbol \varepsilon \nonumber \\
&amp; = &amp;
- \frac{n}{2}ln(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} (\mathbf{Y} - \mathbf{X} \boldsymbol \beta)&#39;(\mathbf{Y} - \mathbf{X} \boldsymbol \beta) \nonumber \\
&amp; = &amp;
- \frac{n}{2}ln(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} (\mathbf{Y&#39;Y} - 2\mathbf{Y&#39;X} \boldsymbol \beta + \boldsymbol \beta&#39; \mathbf{X&#39;X} \boldsymbol \beta)
   \tag{13.19}
\end{eqnarray}\]</span></p>
<p>Establecida la función de verosimilitud, el siguiente paso consisten en la estimación de los parámetros. Para tal efecto debemos determinar las condiciones de primer orden, quedando de la siguiente forma:
<span class="math display" id="eq:emq26">\[\begin{equation}
\frac{\partial l(\boldsymbol{\theta} | \boldsymbol{\varepsilon})}{\partial \hat{\boldsymbol \beta}} = -2{\mathbf{X&#39;Y}} + 2{\mathbf{X&#39;X}} \hat{\boldsymbol{\beta}} = \mathbf{0}
   \tag{13.20}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:emq27">\[\begin{equation}
\frac{\partial l(\boldsymbol{\theta} | \boldsymbol{\varepsilon})}{\partial \hat{\sigma}^2} = - \frac{n}{2 \sigma^2} + \frac{1}{2 (\sigma^2)^2} (\mathbf{Y} - \mathbf{X} \hat{\boldsymbol \beta})&#39;(\mathbf{Y} - \mathbf{X} \hat{\boldsymbol \beta}) = \bf{0}
\tag{13.21}
\end{equation}\]</span></p>
<p>De las dos ecuaciones anteriores podemos deducir las fórmulas de nuestros estimadores de Máxima Verosimilitud:
<span class="math display" id="eq:emq28">\[\begin{equation}
\hat{\boldsymbol{\beta}} = (\mathbf{X&#39;X})^{-1}\mathbf{X&#39;Y}
\tag{13.22}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:emq29">\[\begin{equation}
\hat{\sigma}^2 = \frac{\mathbf{e&#39;e}}{n}
\tag{13.23}
\end{equation}\]</span></p>
<p>El procedimiento de de máxima verosimilitud es el más atractivo de los demás procedimientos de estimación, ya que sus propiedades asíntoticas son que:</p>
<p><em>Un estimador es asíntoticamente eficiente si éste es consistente, asíntoticamente distribuido de forma normal y posee una matriz de varianza y covarianza que no es más grande que la matrix de varianzas y covarianzas asociadas a cualquier otro estimador.</em></p>
<p>Si se asume que la función de densidad conjunta cumple con las condiones de regularidad (que la primer derivada del logarítmo de la función de verosimilitud es continua en todo punto, y que las condiciones de primer órden y segundo órden son conocidas), podemos enunciar el suiguente:</p>
<p><strong>Teorema. Propiedades de un Estimador de Máxima Verosimilitud}. Bajo condiciones de regularidad, el estimador de máxima verosimilitud posee las siguientes propiedades asíntoticas:</strong></p>
<ol style="list-style-type: decimal">
<li>Consistencia: <span class="math inline">\(plim \hat{\boldsymbol{\theta}} = \boldsymbol{\theta}_0\)</span></li>
<li>Normalidad asíntotica: <span class="math inline">\(\hat{\boldsymbol{\theta}} \sim N[\boldsymbol{\theta}_0, I(\boldsymbol{\theta}_0)^{-1}]\)</span>, donde
<span class="math display">\[
I(\boldsymbol{\theta}_0) = -E_0[\partial^2 ln(L)/\partial \boldsymbol{\theta}_0 \partial \boldsymbol{\theta}&#39;_0]
\]</span></li>
<li>Eficiencia asíntotica: <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> es asíntoticamente eficiente y alcanza la cota inferior de Cramér-Rao.</li>
<li>Invarianza. El estimador de máxima verosimilitud de <span class="math inline">\(\boldsymbol{\gamma}_0 = c(\boldsymbol{\theta}_0)\)</span> es <span class="math inline">\(c(\hat{\boldsymbol{\theta}})\)</span> si <span class="math inline">\(c(\boldsymbol{\theta}_0)\)</span> es una función continúa y diferenciable.</li>
</ol>
</div>
<div id="métricas-de-bondad-de-ajuste" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> Métricas de bondad de ajuste</h2>
<p>El criterio que dio origen a los estimadores de MCO consiste en el valor mínimo para la suma del cuadrado de todos los residuales. Esta suma es, por otro lado, una medida de ajuste de la línea de regresión a los datos. Sin embargo, esta medida puede ser facilmente alterada y, por lo tanto, rescalada por una simple multiplicación de los residuales por cualquier valor. Recordemos que el valor de los residuales esta basado en los valores de <span class="math inline">\(\bf{X}\)</span>, así podríamos pregntarnos por cuanto de la variación de <span class="math inline">\(\bf{Y}\)</span> es explicada por la varación de <span class="math inline">\(\bf{X}\)</span>.</p>
<p>De la Figura 1 podemos afirmar que la variación total de la variable dependiente <span class="math inline">\(\bf{Y}\)</span> se puede descomponer en dos partes, es decir, la variación total de <span class="math inline">\(\bf{Y}\)</span> se puede expresar como la suma dada por:</p>
<p><span class="math display">\[SST = \sum^{n}_{i=1}{(y_i - \bar{y})^2} \]</span></p>
<p>Dada la deficnición de regresión tenemos que <span class="math inline">\(\bf{Y} = \bf{X} \hat{\boldsymbol{\beta}} + \bf{e} = \hat{\bf{Y}} + \bf{e}\)</span>. Es decir, <span class="math inline">\(y_i = \hat{y}_i + e_i = {\bf{X}}_i \hat{\boldsymbol{\beta}} + e_i\)</span>. De donde podemos inferir que:</p>
<p><span class="math display">\[ y_i - \bar{y} = \hat{y}_i - \bar{y} + e_i = ({\bf{X}}_i - \bar{\bf{X}}_i )\hat{\boldsymbol{\beta}} + e_i \]</span></p>
<p>Se definimos <span class="math inline">\(\bf{M^0}\)</span> como una matriz saca promedios (con las propiedad de ser idempotente y simetrica) y definida como:</p>
<p><span class="math display">\[
\bf{M^0}=
\left[ \begin{array}{ccccc}
1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \ldots &amp; 1
\end{array} \right]
-
\left[ \begin{array}{ccccc}
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1
\end{array} \right]
\]</span>
<span class="math display">\[=
\bf{I}_n
-
\left[ \begin{array}{c}
1  \\
1 \\
\vdots \\
1
\end{array} \right]  
\left[ \begin{array}{ccccc}
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1 \\
\end{array} \right]
=
\bf{I}_n - i&#39;i
\]</span></p>
<p>De tal forma que para cualquier vector o matriz, <span class="math inline">\(\bf{W}\)</span>, sucede que: <span class="math inline">\(\bf{M^0}\bf{W}= \bf{W} - \bar{\bf{W}}\)</span>, por ello le llammos matriz saca promedios. Regresando a nuestra discusión, podemos escribir que:</p>
<p><span class="math display">\[\bf{Y} - \bar{\bf{Y}} = \bf{M^0Y} = \bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{M^0e}\]</span></p>
<p>Recordemos que si <span class="math inline">\(\bf{M^0}\)</span> extrae los promedios, estonces <span class="math inline">\(\bf{M^0e} = e\)</span>. Así podemos verificar que el producto <span class="math inline">\(\bf{Y&#39;M^0}\bf{M^0Y}\)</span> es igual a:</p>
<p><span class="math display">\[SST = ({\bf{Y}} - \bar{\bf{Y}})&#39;({\bf{Y}} - \bar{\bf{Y}}) = \sum^{n}_{i=1}{(y_i - {\bar{y}})^2} = \bf{Y&#39;M^0Y}\]</span></p>
<p><span class="math display">\[\bf{Y&#39;M^0Y} = Y&#39;(\bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{M^0e}) = (\bf{X} \hat{\boldsymbol{\beta}} + \bf{e})&#39;(\bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{M^0e})\]</span>
<span class="math display">\[= (\hat{\boldsymbol{\beta}}&#39; \bf{X&#39;} + \bf{e&#39;})(\bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{M^0e}) \]</span>
<span class="math display">\[= \hat{\boldsymbol{\beta}}&#39; \bf{X&#39;}\bf{M^0X} \hat{\boldsymbol{\beta}} + \hat{\boldsymbol{\beta}}&#39; \bf{X&#39;}\bf{M^0e} + \bf{e&#39;}\bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{e&#39;}\bf{M^0e}\]</span></p>
<p>Finlamente, como recordaran de clases pasadas, dijimos que sólo cuando nuestra regresión inclui constante la suma de residuales <span class="math inline">\(\sum^{n}_{i=1}{e_i} = 0\)</span>. De esta forma, el promedio de residuales <span class="math inline">\((\sum^{n}_{i=1}{e_i})/n = 0\)</span>. Es decir, que nuestra matriz saca promedio multiplicada por el vector de residuales es igual a <span class="math inline">\(\bf{M^0e} = e - \bar{e} = e\)</span>. Así:</p>
<p><span class="math display">\[\bf{Y&#39;M^0Y} = \hat{\boldsymbol{\beta}}&#39; \bf{X&#39;}\bf{M^0X} \hat{\boldsymbol{\beta}} + \hat{\boldsymbol{\beta}}&#39; \bf{X&#39;}\bf{e} + \bf{e&#39;}\bf{X} \hat{\boldsymbol{\beta}} + \bf{e&#39;}\bf{e}\]</span></p>
<p>Por otro lado, sabemos que la solución por el método de MCO garantiza que el producto de <span class="math inline">\(\bf{X&#39;e} = \bf{e&#39;X} = 0\)</span>.</p>
<p><span class="math display">\[SST = \bf{Y&#39;M^0Y} = \hat{\boldsymbol{\beta}}&#39; \bf{X&#39;}\bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{e&#39;}\bf{e}\]</span>
<span class="math display">\[SST = SSR + SSE\]</span></p>
<p>O en palabras, la variavilidad total de <span class="math inline">\(\bf{Y}\)</span> se puede descomponer en dos: la variavilidad originada por la regresión y la variavilidad que no puede ser explicada, es decir, la del término de error.</p>
<p>Dicho esto, porponemos el siguiente coeficiente de bondad de ajuste a los datos, el cual suele concerse como <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[R^2 = \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST}\]</span></p>
<p>En fórmula:</p>
<p><span class="math display">\[R^2 = 1 - \frac{\bf{e&#39;e}}{\bf{Y&#39;M^0Y}}\]</span></p>
<p>Por último el <span class="math inline">\(R^2\)</span> ajustado solo es:</p>
<p><span class="math display">\[R^2 = 1 - \frac{{\bf{e&#39;e}}/(n-K)}{{\bf{Y&#39;M^0Y}}/(n-1)}\]</span></p>
</div>
<div id="pruebas-de-hipótesis" class="section level2" number="13.4">
<h2><span class="header-section-number">13.4</span> Pruebas de Hipótesis</h2>
<p>El Análisis de Regresión se suele usar con mucha frecuencia para los siguientes propósitos: la estimación y predicción, y para probar algún tipo de hipótesis. La estimación y predicción se analizará con mayor detalle al final de esta clase y el sesiones futuras. Por lo que respecta a las pruebas de hipótesis estableceremos los siguiente.</p>
<p>Recordemos que nuestro modelo general de regresión está dado por la siguiente expresión:</p>
<p><span class="math display">\[{\bf Y} = {\bf X}{\boldsymbol \beta} + {\boldsymbol \varepsilon}\]</span></p>
<p>Ahora consideremos un ejemplo, supongamos que desea plantear una regresión del tipo logarítmica con el objeto de determinar la demanda de tabaco. Así, establece la siguiente relación:</p>
<p><span class="math display">\[ln(Q_{Tabaco}) = \beta_0 + \beta_1 ln(P_{Tabaco}) + \beta_2 ln(P_{Alcohol}) + \beta_3 ln(Ingreso) + \varepsilon\]</span></p>
<p>Donde <span class="math inline">\(Q_{Tabaco}\)</span> es la cantidad de tabaco demandada, y <span class="math inline">\(P_{Tabaco}\)</span> y <span class="math inline">\(P_{Alcohol}\)</span> son el precio del tabaco y del aocohol, respectivamente. Suponga, adicionalmente, que sospecha que el tabaco y el alcohol guardan una relación de complementariedad, por lo que espera que los paramétros asociados a las variables de precios de ambos tengan el mismo signo (-). Asimismo, suponga que estas variables son las únicas relevantes para este caso y el resto de la información es no observable o no medible.</p>
<p>Supongamos, quizá de forma absurda, que el tabaco y el alcohol exhiben una elásticidad unitaria, por lo que decide plantear la siguiente hipótesis:</p>
<blockquote>
<p><span class="math inline">\(H_0: \beta_1 = 1\)</span> y <span class="math inline">\(\beta_2 = 1\)</span></p>
</blockquote>
<blockquote>
<p><span class="math inline">\(H_1: No\)</span> <span class="math inline">\(H_0\)</span></p>
</blockquote>
<p>La hipótesis nula es equivalente a escribir el siguiente sistema de ecuaciones:</p>
<p><span class="math display">\[0 \beta_0 + 1 \beta_1 + 0 \beta_2 + 0 \beta_3 = 1\]</span>
<span class="math display">\[0 \beta_0 + 0 \beta_1 + 1 \beta_2 + 0 \beta_3 = 1\]</span></p>
<p>El cual podemos escribir como:</p>
<p><span class="math display">\[
\left(
\begin{array}{c c c c}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
\end{array}
\right)
\left(
\begin{array}{c}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3 \\
\end{array}
\right)
=
\left(
\begin{array}{c}
1 \\
1 \\
\end{array}
\right)
\]</span></p>
<p>En forma reducida:</p>
<p><span class="math display">\[{\bf R} {\boldsymbol \beta} = q\]</span></p>
<p>Con lo cual la hipótesis original se puede escribir como:</p>
<p><span class="math display">\[H_0: {\bf R} {\boldsymbol \beta} = q\]</span></p>
<p><span class="math display">\[H_1: {\bf R} {\boldsymbol \beta} \ne q\]</span></p>
<p>Observemos que podemos afirmar que la hipótesis tiene dos restricciones. Regresando a nuestro caso general:</p>
<p><span class="math display">\[{\bf Y} = {\bf X}{\boldsymbol \beta} + {\boldsymbol \varepsilon}\]</span></p>
<p>Por analogía podríamos escribir un conjunto de ``J’’ restricciones como:</p>
<p><span class="math display">\[r_{10} \beta_0 + r_{11} \beta_1 + \ldots + r_{1K} \beta_K = q_1\]</span>
<span class="math display">\[r_{20} \beta_0 + r_{21} \beta_1 + \ldots + r_{2K} \beta_K = q_2\]</span>
<span class="math display">\[\vdots\]</span>
<span class="math display">\[r_{J0} \beta_0 + r_{J1} \beta_1 + \ldots + r_{JK} \beta_K = q_J\]</span></p>
<p>Sistema que se puede escribir en forma matricial como:</p>
<p><span class="math display">\[
\left(
\begin{array}{c c c c}
r_{10} &amp; r_{11} &amp; \ldots &amp; r_{1K} \\
r_{20} &amp; r_{21} &amp; \ldots &amp; r_{2K} \\
\vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
r_{J0} &amp; r_{J1} &amp; \ldots &amp; r_{JK} \\
\end{array}
\right)
\left(
\begin{array}{c}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_3 \\
\end{array}
\right)
=
\left(
\begin{array}{c}
q_1 \\
q_2 \\
\vdots \\
q_J \\
\end{array}
\right)
\]</span></p>
<p>De esta forma podemos, finalmente, escribir una hipótesis general:</p>
<blockquote>
<p><span class="math display">\[H_0: {\bf R} {\boldsymbol \beta} = q\]</span></p>
</blockquote>
<blockquote>
<p><span class="math inline">\(H_1: No\)</span> <span class="math inline">\(H_0\)</span></p>
</blockquote>
<p>Dicho lol anterior, el restante argumento versa sobre cómo hacer uso de estas restricciones conjuntas.</p>
<div id="prueba-f" class="section level3" number="13.4.1">
<h3><span class="header-section-number">13.4.1</span> Prueba F</h3>
<p>Cuando deseamos evaluar una hipótesis con más de una restricción se debe ocupar la prueba F. La cual se puede escribir como:</p>
<p><span class="math display">\[F_{[J, n-K]} = \frac{({\bf R}{\boldsymbol \beta} - {\bf q})&#39; [s^2 {\bf R} ({\bf X&#39;X})^{-1} {\bf R}&#39;] ({\bf R}{\boldsymbol \beta} - {\bf q})}{J}\]</span></p>
<p>Esta estadística se distribuye como una F de Fisher con <span class="math inline">\(J\)</span> y <span class="math inline">\(n-K\)</span> grados de libertad.</p>
</div>
<div id="prueba-t" class="section level3" number="13.4.2">
<h3><span class="header-section-number">13.4.2</span> Prueba t</h3>
<p>Cuando deseamos evaluar una hipótesis con solo una restricción se debe ocupar la prueba t. La cual se puede escribir como:</p>
<p><span class="math display">\[t_{[n-K, \alpha/2]} = ({\bf R}{\boldsymbol \beta} - {\bf q})&#39; [s^2 {\bf R} ({\bf X&#39;X})^{-1} {\bf R}&#39;] ({\bf R}{\boldsymbol \beta} - {\bf q})\]</span></p>
<p>Esta estadística se distribuye como una t con <span class="math inline">\(n-K\)</span> grados de libertad.</p>

</div>
</div>
</div>
  </main>

  <div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page">
      <h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          <li><a id="book-source" href="#">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="#">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
      </div>
    </nav>
  </div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5">
  <div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Notas de Clase: Series de Tiempo</strong>" was written by Benjamín Oliva, Omar Alfaro-Rivera y Emiliano Pérez Caullieres. It was last built on 2022-08-07.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
<script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>

</html>
